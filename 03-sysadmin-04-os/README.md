# Домашнее задание к занятию "3.4. Операционные системы, лекция 2"

## Модуль 3. Основы системного администрирования

## Студент: Иван Жиляев

Ответы на вопросы из задания:

1. >На лекции мы познакомились с [node_exporter](https://github.com/prometheus/node_exporter/releases). В демонстрации его исполняемый файл запускался в background. Этого достаточно для демо, но не для настоящей production-системы, где процессы должны находиться под внешним управлением. Используя знания из лекции по systemd, создайте самостоятельно простой [unit-файл](https://www.freedesktop.org/software/systemd/man/systemd.service.html) для node_exporter:
   >
   > * поместите его в автозагрузку,
   > * предусмотрите возможность добавления опций к запускаемому процессу через внешний файл (посмотрите, например, на `systemctl cat cron`),
   > * удостоверьтесь, что с помощью systemctl процесс корректно стартует, завершается, а после перезагрузки автоматически поднимается.

   Сперва требуется скачать `node_exporter` и распаковать его:

   ```
   wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz
   tar zxvf node_exporter-1.0.1.linux-amd64.tar.gz
   ```

   Затем создаём unit-файл по пути `/lib/systemd/system/node_exporter.service` с таким содержимым:

   ```
   [Unit]
   Description=Node Exporter Service
   After=network.target

   [Service]
   EnvironmentFile=/home/vagrant/node_exporter-1.0.1.linux-amd64/node_exporter_options
   ExecStart=/home/vagrant/node_exporter-1.0.1.linux-amd64/node_exporter $EXTRA_OPTS
   Restart=on-failure

   [Install]
   WantedBy=multi-user.target
   ```

   Не забудем создать указанный в юните конфигурационный файл и сразу добавим в него отличную от умолчаний опцию:

   ```
   vagrant@vagrant:~/node_exporter-1.0.1.linux-amd64$ echo EXTRA_OPTS=--web.listen-address=\":9102\" > node_exporter_options
   ```

   Теперь включаем автозагрузку сервиса и запускаем его командами:

   ```
   systemctl enable node_exporter.service
   systemctl start node_exporter.service
   ```

   Проверяем количество доступных метрик:

   ```
   vagrant@vagrant:~/node_exporter-1.0.1.linux-amd64$ curl --silent localhost:9102/metrics | grep -v "#" -c
   456
   ```

   Проверка показала, что служба корректно управляется через команды `systemctl start|stop|restart node_exporter`, принимает конфигурацию из внешнего файла, а также самостоятельно стартует после перезагрузки хоста.

1. >Ознакомьтесь с опциями node_exporter и выводом `/metrics` по-умолчанию. Приведите несколько опций, которые вы бы выбрали для базового мониторинга хоста по CPU, памяти, диску и сети.

   Для базового мониторинга указанных ресурсов я бы выбрал следующие метрики:

   - __для процессора:__

      метрика `node_cpu_seconds_total` берёт данные из `/proc/stat`, т.е. показывает общее затраченное время на определённые категории процессов с момента запуска системы. 
      
      - `node_cpu_seconds_total{mode="iowait"}` - покажет не упираемся ли мы в лимит по I/O
      - `node_cpu_seconds_total{mode="idle"}` - через инверсию этой метрики можно оценить суммарную загрузку процессора
      

   - __для памяти:__
      
      семейство метрик `node_memory_` содержит информацию о распределении памяти на хосте. Базово стоит наблюдать за:
      
      - `node_memory_MemFree_bytes` - свободная память
      - `node_memory_MemAvailable_bytes` - доступная для использования память

   - __для диска:__
      
      - `node_filesystem_avail_bytes` - доступное место на ФС
      - `node_filesystem_files_free` - оценка количества свободных inode в ФС

   - __для сети:__
      
      - `node_netstat_Tcp_ActiveOpens` - смотрим за количеством открытых tcp-сессий
      - `node_network_receive_bytes_total` - байт получено
      - `node_network_receive_packets_total` - пакетов получено
      - `node_network_transmit_bytes_total` - байт отправлено
      - `node_network_transmit_packets_total` - пакетов отправлено

1. >Установите в свою виртуальную машину [Netdata](https://github.com/netdata/netdata). Воспользуйтесь [готовыми пакетами](https://packagecloud.io/netdata/netdata/install) для установоки (`sudo apt install -y netdata`). После успешной установки:
   > * в конфигурационном файле `/etc/netdata/netdata.conf` в секции [web] замените значение с localhost на `bind to = 0.0.0.0`,
   > * добавьте в Vagrantfile проброс порта Netdata на свой локальный компьютер и сделайте `vagrant reload`:
   >
   > ```bash
   > config.vm.network "forwarded_port", guest: 19999, host: 19999
   > ```
   >
   > После успешной перезагрузки в браузере *на своем ПК* (не в виртуальной машине) вы должны суметь зайти на `localhost:19999`. Ознакомьтесь с метриками, которые по умолчанию собираются Netdata и с комментариями, которые даны к этим метрикам.

   Всё получилось. Мне кажется, что это - отличный вариант "штучного" мониторинга!  
   Полагаю, что его стоит использовать не вместо, а вместе с централизованными системами (как zabbix) для отладки тестовых стендов или углублённого runtime-мониторинга хостов.

1. >Можно ли по выводу `dmesg` понять, осознает ли ОС, что загружена не на настоящем оборудовании, а на системе виртуализации?

   Да, в самом начале вывода команды `dmesg -H -f kern` имеется строка, обозначающая запуск системы на гипервизоре:

   ```
   [  +0.000000] Hypervisor detected: KVM
   ```

1. >Как настроен sysctl `fs.nr_open` на системе по-умолчанию? Узнайте, что означает этот параметр. Какой другой существующий лимит не позволит достичь такого числа (`ulimit --help`)?

   Для проверки значения этого параметра выполним такой запрос:

   ```
   root@vagrant:~# sysctl -n fs.nr_open
   1048576
   ```

   Данный параметр задаёт лимит на количество открытых дескрипторов в системе, т.е. максимальное число открытых файлов.

   В `ulimit --help` есть упоминание более низкого "sotf"-ового лимита, задаваемого опцией `-n`:

   ```
   vagrant@vagrant:~$ ulimit -a | grep "\-n"
   open files                      (-n) 1024
   ```

1. >Запустите любой долгоживущий процесс (не `ls`, который отработает мгновенно, а, например, `sleep 1h`) в отдельном неймспейсе процессов; покажите, что ваш процесс работает под PID 1 через `nsenter`. Для простоты работайте в данном задании под root (`sudo -i`). Под обычным пользователем требуются дополнительные опции (`--map-root-user`) и т.д.

   В работе использую `tmux`. В одном его окне запустил команду:

   ```
   root@vagrant:~# unshare --fork --pid --mount-proc sleep 1h
   ```

   В другом при этом видна такая структура процессов:

   ```
   vagrant     2006 Ss   tmux
   vagrant     2007 Ss    \_ -bash
   root        2098 S     |   \_ sudo -i
   root        2099 S     |       \_ -bash
   root        2486 R+    |           \_ ps axfo user,pid,stat,command
   vagrant     2014 Ss    \_ -bash
   root        2021 S         \_ sudo -i
   root        2023 S             \_ -bash
   root        2443 S+                \_ unshare --fork --pid --mount-proc sleep 1h
   root        2444 S+                    \_ sleep 1h
   ```

   Теперь убедимся что внутри нового namespace-а своя иерархия процессов:

   ```
   root@vagrant:~# nsenter -t 2444 --pid --mount ps auxf
   USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
   root           2  0.0  0.6  11484  3352 pts/1    R+   20:05   0:00 ps auxf
   root           1  0.0  0.1   8076   588 pts/2    S+   19:57   0:00 sleep 1h
   ```

1. >Найдите информацию о том, что такое `:(){ :|:& };:`. Запустите эту команду в своей виртуальной машине Vagrant с Ubuntu 20.04 (**это важно, поведение в других ОС не проверялось**). Некоторое время все будет "плохо", после чего (минуты) – ОС должна стабилизироваться. Вызов `dmesg` расскажет, какой механизм помог автоматической стабилизации. Как настроен этот механизм по-умолчанию, и как изменить число процессов, которое можно создать в сессии?

   Это - "логическая бомба". Описание: в приведённой строке мы сперва определяем функцию, которая вызывает через пайп себя дважды в background-е, а затем просто исполняем её. Работу бомбы обеспечивает свойсто пайпа запускать команды параллельно в subshell-ах, что даёт рост порождаемых процессов в геометрической прогрессии с знаменателем `2`.

   Из вывода `dmesg` к теме вопроса подходит только эта строка:

   ```
   cgroup: fork rejected by pids controller in /user.slice/user-1000.slice/session-3.scope
   ```

   Получается, что некий инструмент ядра `pids controller` (думаю, речь об [этом](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/pids.html)) решил что есть проблема и запретил систмемные вызовы `fork` в соответствующей `cgroup`.

   Изменить максимально возможное число процессов можно с помощью `ulimit -u` или изменив значение в этом файле:

   ```
   root@vagrant:~# cat /sys/fs/cgroup/pids/user.slice/user-1000.slice/pids.max 
   1121
   ```

   По мониторингу `Netdata` видно, что сработало ограничение пространства имён - в момент работы бомбы под пользователем было запущено 1118 процессов.

   В целом это задание осталось непонятным:
   
   - Как запрет системных вызовов `fork` помог? Ведь в системе уже наплодилось максимально допустимое количество процессов и кто их почистил (после стабилизации лишних процессов нет)?

   - Почему бомба не работает если не помещать выполнение функции в background? Ведь для конвейера в любом случае будут использоваться subshell-ы.

   - Верно ли я понял что `ulimit` (как built-in bash-а) и файл `/sys/fs/cgroup/pids/user.slice/user-1000.slice/pids.max` (ограничение для `namespace`) являются разными, не связанными друг с другом инструментамми? 

   - Почему на сессию под `root` влияет запущенная под `vagrant` бомба? Единый (наследуемый) namespace по-умолчанию?  
   Попробовал сделать новые namespace-ы командой под рутом `unshare --fork --pid --cgroup --mount-proc /bin/bash` и запустить бомбу - страдают также все сессии в системе.

   _Прим.: Буду признателен наводке на хороший материал по концепции namespace-ов и практической работе с ними, а то данных из лекции мне не хватило для хорошего усвоения, да и в сети пока ничего путного не встретил._
